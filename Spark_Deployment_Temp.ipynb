{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BQu8BQDJRTzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "IubsuHn3RTxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f0ip2WksaKq",
        "outputId": "84cc86d2-9104-4ddf-91f4-c48c7c7f96a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/221.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/221.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.7\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVSEk4MlsaNk",
        "outputId": "2229de07-db0b-4be9-a134-3788a9a4776b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=1b630eb40941206e0a5d2f94e1a42a54ba50a56bf8edcff93a7e62d78f5db157\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYGTh27jsddT",
        "outputId": "8e9485aa-3645-4b9c-8cbd-99e5cb83bf33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,520 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,535 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,294 kB]\n",
            "Fetched 4,690 kB in 3s (1,428 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "15 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pJiTYQlb-EH6"
      },
      "outputs": [],
      "source": [
        "#!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ermu-yzmQZoY"
      },
      "outputs": [],
      "source": [
        "#!pip install moviepy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3ZrCX53_ssHy"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import cv2\n",
        "import os\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "#from ultralytics import YOLO\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "import moviepy.editor as mp\n",
        "import json\n",
        "import re\n",
        "#import gradio as gr\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmWMJ_FeJDyD"
      },
      "source": [
        "# Full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YMDeEOwLtGYN"
      },
      "outputs": [],
      "source": [
        "def video_transcription(video_path):\n",
        "  try:\n",
        "    model = whisper.load_model('medium')\n",
        "    transcript = model.transcribe(video_path, verbose = False, language = 'en')\n",
        "\n",
        "    # JSON Dump (Find a way to not create a file and just dump into a variable or something)\n",
        "    #json_file_path = video_path.split('/')[-1][:-4]+ \".json\"\n",
        "    #with open(json_file_path, 'w') as json_file:\n",
        "    #json.dump(transcript, json_file, indent = 2)\n",
        "    #json_file_path = video_path.split('/')[-1][:-4]+ \".json\"\n",
        "    #with open(json_file_path, 'w') as json_file:\n",
        "    return json.dumps(transcript)\n",
        "\n",
        "  except Exception as e:\n",
        "    return e\n",
        "\n",
        "def action_detection(json_object, openai_key=\"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\"):\n",
        "  #try:\n",
        "    # JSON Dump (Find a way to not create a file and just dump into a variable or something)\n",
        "    #with open(json_path, 'r') as f:\n",
        "    transcript = json.loads(json_object)\n",
        "    #print(transcript)\n",
        "    transcript_string = ''\n",
        "    for segments in transcript['segments']:\n",
        "      transcript_string+=str(segments['text']+'\\n') #str(segments['id'])+\n",
        "\n",
        "    chunks = []\n",
        "    output = {}\n",
        "    count = 0\n",
        "    split_transcript = transcript_string.split(\"\\n\")\n",
        "    #print(split_transcript[33])\n",
        "    #print(len(split_transcript[33]))\n",
        "    num_lines = len(split_transcript)\n",
        "    num_chars = 0\n",
        "    # for i in split_transcript:\n",
        "    #   num_chars+=len(i)\n",
        "    # print(num_chars)\n",
        "    # print(num_lines)\n",
        "    i = 0\n",
        "    prev = 0\n",
        "\n",
        "    while i < num_lines:\n",
        "      num_chars+=len(split_transcript[i])\n",
        "      #print(num_chars)\n",
        "      if num_chars>=16000:\n",
        "        chunks.append(\"\\n\".join(split_transcript[prev:i]))\n",
        "        prev = i\n",
        "        num_chars = 0\n",
        "      i+=1\n",
        "      if i == num_lines:\n",
        "        chunks.append(\"\\n\".join(split_transcript[prev:i]))\n",
        "\n",
        "    # print(chunks[0])\n",
        "    # print()\n",
        "    # print()\n",
        "    # print()\n",
        "    # print(chunks[1])\n",
        "    #for i in chunks:\n",
        "    #  print(i)\n",
        "    #print(len(chunks))\n",
        "    #for i in range(0, num_lines - 100, 100):\n",
        "    #  chunks.append(split_transcript[i:i+100])\n",
        "    client = OpenAI(api_key = openai_key)\n",
        "\n",
        "    for i in chunks:\n",
        "      completion = client.chat.completions.create(\n",
        "      model=\"gpt-4\",\n",
        "        #  messages=[\n",
        "        #   {\"role\": \"system\", \"content\": f\"Given this {transcript_string} You are an AI system specialized in detecting planning issues, critiquing plans, and analyzing conversations between people regarding how to disperse. Additionally, identify any instances suggesting 1st Amendment violations or officers expressing the belief that this protest was anti-police. Finally, flag any aggressive comments found in the audio transcript.\"},\n",
        "        #   {\"role\": \"user\", \"content\":\"Give responce like this following examples: Sentence: '18: What do you got?' Explanation: This sentence may indicate confusion or a need for clarification, as the speaker is asking for information. It could potentially be a planning issue if the speaker is seeking information to execute a specific task.\"}\n",
        "        # ]\n",
        "\n",
        "       messages=[\n",
        "         {\"role\": \"user\", \"content\": f\"You are an AI system specialized in detecting planning issues, critiquing plans, and analyzing conversations between police officers regarding how to disperse. Additionally, identify any instances suggesting 1st Amendment violations, criticizing the lack of a plan, and aggressive comments. Transcript:\\n\\n{i}\\n\\n\"},\n",
        "         {\"role\": \"user\", \"content\": \"Give response only in the json format for example: \\{\\\"1\\\":  \\\"What should we do now. I don't have a clue?\\\", \\\"2\\\": \\\"what the fuck is this\\\", \\\"3\\\":\\\"Beat the fuck out of them\\\"\\}. There can be multiple instances, find out all of them. If you do not find anything just return {\\\"None\\\":\\\"None\\\"}\"}\n",
        "       ]\n",
        "      )\n",
        "\n",
        "\n",
        "      gpt_output = completion.choices[0].message.content\n",
        "      #return gpt_output\n",
        "      gpt_output = dict(json.loads(gpt_output))\n",
        "      #return gpt_output\n",
        "      for j in gpt_output.values():\n",
        "       output[count] = j\n",
        "       count+=1\n",
        "\n",
        "      #return output\n",
        "\n",
        "      #output = json.loads(output)\n",
        "\n",
        "    # paragraphs = re.split(r'\\n\\n', output)\n",
        "\n",
        "    # sentences = []\n",
        "    # explanations = []\n",
        "\n",
        "    # for paragraph in paragraphs:\n",
        "    #     sentence_match = re.search(r\"Sentence: '(.+)'\", paragraph)\n",
        "    #     explanation_match = re.search(r\"Explanation: (.+)\", paragraph)\n",
        "\n",
        "    #     if sentence_match and explanation_match:\n",
        "    #         sentences.append(sentence_match.group(1).split(': ')[-1])\n",
        "    #         explanations.append(explanation_match.group(1))\n",
        "\n",
        "    #for i in range(len(sentences)):\n",
        "    #    print(f\"Sentence: '{sentences[i]}'\")\n",
        "    #    print(f\"Explanation: {explanations[i]}\\n\")\n",
        "\n",
        "    sent_with_time = []\n",
        "\n",
        "    for sentence_to_search in out.values():\n",
        "        pattern = re.compile(re.escape(sentence_to_search), re.IGNORECASE)\n",
        "\n",
        "        matching_entries = [entry for entry in transcript['segments'] if re.search(pattern, entry['text'])]\n",
        "\n",
        "        if matching_entries:\n",
        "            for entry in matching_entries:\n",
        "                sent_with_time.append(sentence_to_search + ' Start Time: ' + str(entry['start']) + ' End Time: ' + str(entry['end']))\n",
        "\n",
        "    # # #output_dict = json.loads(output)\n",
        "\n",
        "    return sent_with_time\n",
        "\n",
        "  #except Exception as e:\n",
        "  #  return e\n",
        "\n",
        "def process_video(video_path, weights):\n",
        "    try:\n",
        "        # This code cell detects batons in the video\n",
        "        current_frame = 0\n",
        "        model = YOLO(weights)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        conseq_frames = 0\n",
        "        start_time = \"\"\n",
        "        end_time = \"\"\n",
        "        res = []\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Detecting baton on one frame per second\n",
        "            if current_frame % fps == 0:\n",
        "                currect_sec = current_frame/fps\n",
        "\n",
        "                # Model prediction on current frame\n",
        "                results = model(frame, verbose = False)\n",
        "                count = 0\n",
        "                classes = results[0].boxes.data\n",
        "\n",
        "                # Formatting the time for printing\n",
        "                hours, remainder = divmod(currect_sec, 3600)\n",
        "                minutes, seconds = divmod(remainder, 60)\n",
        "                hours = str(int(hours)).zfill(2)\n",
        "                minutes = str(int(minutes)).zfill(2)\n",
        "                seconds = str(int(seconds)).zfill(2)\n",
        "\n",
        "                for i in classes:\n",
        "\n",
        "                   # Checking if baton is detected (i.e. if the class corresponding to baton is 1 or not)\n",
        "                    if float(i[5]) == 1:\n",
        "                        count+=1\n",
        "\n",
        "                # Marking the start_time if this is the first consecutive frame a baton is detected in\n",
        "                if count >= 1:\n",
        "                    conseq_frames+=1\n",
        "                    if conseq_frames == 1:\n",
        "                        start_time = hours + \":\" + minutes + \":\" + seconds\n",
        "\n",
        "                # Marking the end time if after one or multiple consecutive frames of detection, a baton is not detected\n",
        "                else:\n",
        "                    if conseq_frames > 0:\n",
        "                        conseq_frames = 0\n",
        "                        end_time = hours + \":\" + minutes + \":\" + seconds\n",
        "\n",
        "                        # Printing time intervals in which baton was detected\n",
        "                        res.append(start_time + \" to \" + end_time)\n",
        "                        start_time = \"\"\n",
        "                        end_time = \"\"\n",
        "\n",
        "            current_frame += 1\n",
        "        cap.release()\n",
        "\n",
        "        return \"\\n\".join(res)\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        return e\n",
        "\n",
        "# def all_funcs(openai_key,video_path, yolo_weights, pr = gr.Progress(track_tqdm = True)):\n",
        "\n",
        "#   video_path = video_path[0].split('/')[-1]\n",
        "#   yolo_weights = yolo_weights[0].split('/')[-1]\n",
        "#   transcript = video_transcription(video_path)\n",
        "#   sentences = action_detection(json.loads(transcript), openai_key)\n",
        "#   batons = process_video(video_path, yolo_weights)\n",
        "\n",
        "#   print(\"ALL FUNC Executed without errors\")\n",
        "\n",
        "#   return sentences, batons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_dump = video_transcription(\"/content/drive/MyDrive/Spark Project/Data/Civil_Unrest_Tremont_@_Winter-001.mp4\")"
      ],
      "metadata": {
        "id": "eHZxd3sIZXar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AvMeRFubQZoa"
      },
      "outputs": [],
      "source": [
        "out = action_detection(json_dump)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = json.loads(out)"
      ],
      "metadata": {
        "id": "Wci6zAgQC08u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "b30ef3cd-432f-4671-8f46-8c7737361b5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-334ae2866886>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n\u001b[0m\u001b[1;32m    340\u001b[0m                             f'not {s.__class__.__name__}')\n\u001b[1;32m    341\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogatepass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not dict"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeOCvJr2f_GJ",
        "outputId": "ff15e05f-fdd3-4e41-a429-a5cae15721ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: \"Hold on, hold on, we're not forcing them out.\",\n",
              " 1: 'What are you gonna do? Kill the whole fucking civilian?',\n",
              " 2: \"Luckily, we're gonna blow up the place.\",\n",
              " 3: 'Randy! I would come here right the fuck now!',\n",
              " 4: \"You know they'll put you up after class two.\",\n",
              " 5: \"Say they went into your family's house and shot your family on a mistake.\",\n",
              " 6: 'You thought this was gonna catch us, right?'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgKOtbnR2Ahe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "odh_MKheQZoa",
        "outputId": "526aac8e-66a2-4b4a-c40b-cd933c7e6754"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-731b2b418876>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Civil_Unrest_Tremont_@_Winter-001.srt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Civil_Unrest_Tremont_@_Winter-001.srt'"
          ]
        }
      ],
      "source": [
        "transcript = open(\"Civil_Unrest_Tremont_@_Winter-001.srt\").read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI23T92VQZob",
        "outputId": "616c7ed7-615e-4519-a356-d79d8669be52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(transcript[:5000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "ZbZKUNl3Mttf",
        "outputId": "6c0912aa-f2d3-49f7-db58-473905111653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ALL FUNC Executed without errors\n"
          ]
        }
      ],
      "source": [
        "btn = gr.Interface(\n",
        "    fn = all_funcs,\n",
        "    inputs = [\"text\", gr.Files(label = \"Select Video File\"), gr.Files(label = \"Select YOLOv8 Weights File\")],\n",
        "    outputs=[gr.Textbox(label = \"Audio Analysis Time Stamps\", lines = 20), gr.Textbox(label = \"Baton Detection Timestamps\", lines = 20)]\n",
        ")\n",
        "\n",
        "btn.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMF48OxVJHLp"
      },
      "source": [
        "# Baton Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VSlkVeNJQo4"
      },
      "outputs": [],
      "source": [
        "def process_video(video_path, weights):\n",
        "    try:\n",
        "        # This code cell detects batons in the video\n",
        "        current_frame = 0\n",
        "        model = YOLO(weights)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        conseq_frames = 0\n",
        "        start_time = \"\"\n",
        "        end_time = \"\"\n",
        "        res = []\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            # Detecting baton on one frame per second\n",
        "            if current_frame % fps == 0:\n",
        "                currect_sec = current_frame/fps\n",
        "\n",
        "                # Model prediction on current frame\n",
        "                results = model(frame, verbose = False)\n",
        "                count = 0\n",
        "                classes = results[0].boxes.data\n",
        "\n",
        "                # Formatting the time for printing\n",
        "                hours, remainder = divmod(currect_sec, 3600)\n",
        "                minutes, seconds = divmod(remainder, 60)\n",
        "                hours = str(int(hours)).zfill(2)\n",
        "                minutes = str(int(minutes)).zfill(2)\n",
        "                seconds = str(int(seconds)).zfill(2)\n",
        "\n",
        "                for i in classes:\n",
        "\n",
        "                   # Checking if baton is detected (i.e. if the class corresponding to baton is 1 or not)\n",
        "                    if float(i[5]) == 1:\n",
        "                        count+=1\n",
        "\n",
        "                # Marking the start_time if this is the first consecutive frame a baton is detected in\n",
        "                if count >= 1:\n",
        "                    conseq_frames+=1\n",
        "                    if conseq_frames == 1:\n",
        "                        start_time = hours + \":\" + minutes + \":\" + seconds\n",
        "\n",
        "                # Marking the end time if after one or multiple consecutive frames of detection, a baton is not detected\n",
        "                else:\n",
        "                    if conseq_frames > 0:\n",
        "                        conseq_frames = 0\n",
        "                        end_time = hours + \":\" + minutes + \":\" + seconds\n",
        "\n",
        "                        # Printing time intervals in which baton was detected\n",
        "                        res.append(start_time + \" to \" + end_time)\n",
        "                        start_time = \"\"\n",
        "                        end_time = \"\"\n",
        "\n",
        "            current_frame += 1\n",
        "        cap.release()\n",
        "\n",
        "        return \"\\n\".join(res)\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "uSBK_3VBJQmC",
        "outputId": "a18e4eeb-6b99-46c0-99b6-ae3a6e68ae99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://06b7f8c10c60967e6b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://06b7f8c10c60967e6b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with gr.Blocks() as demo:\n",
        "\n",
        "    video_path = gr.Textbox(label = \"Enter Path to Video\")\n",
        "    #openai_keys = gr.Textbox(label = \"Enter your OpenAI Key\")\n",
        "    weights = gr.Textbox(label = \"Enter Path to YOLOv8 Weights\")\n",
        "    #sentences = gr.Textbox(label = \"Sentences Detected\")\n",
        "    batons = gr.Textbox(label = \"Batons Detected\")\n",
        "    btn = gr.Button(value = \"Process Video\")\n",
        "    btn.click(process_video, inputs = [video_path, weights], outputs = batons)\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqdjv7QowrZW"
      },
      "outputs": [],
      "source": [
        "/content/drive/MyDrive/Spark Project/Test_Video.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9-4b-gfrbwa"
      },
      "outputs": [],
      "source": [
        "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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPh9wSJvwvAt"
      },
      "outputs": [],
      "source": [
        "/content/drive/MyDrive/Spark Project/Data (For YOLOv8 Training)/Option 3 - Roboflow (60 Images)/YOLOv8 Best Weights.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvuZJI3-LGOU"
      },
      "outputs": [],
      "source": [
        "process_video(\"/content/drive/MyDrive/Spark Project/Test_Video.mp4\", \"/content/drive/MyDrive/Spark Project/Data (For YOLOv8 Training)/Option 3 - Roboflow (60 Images)/YOLOv8 Best Weights.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7ZZYQp_tbN4"
      },
      "outputs": [],
      "source": [
        "a = video_transcription(\"/content/drive/MyDrive/Spark Project/Test_Video.mp4\")\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtv7izc3HQHP",
        "outputId": "d7fed4ac-3d97-4580-bd21-f698f84f9615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.214-py3-none-any.whl (645 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.5/645.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Collecting thop>=0.1.1 (from ultralytics)\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: thop, ultralytics\n",
            "Successfully installed thop-0.1.1.post2209072238 ultralytics-8.0.214\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}